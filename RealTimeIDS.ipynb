{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Install and import the necessary libraries**"
      ],
      "metadata": {
        "id": "UogJSKKQRg5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67jnjyiXNmdW"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf openai gradio langchain\n",
        "!pip install langchain_openai\n",
        "!pip install PyPDF2\n",
        "!pip install -U langchain-community\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Replace with the raw file download link\n",
        "zip_file_url = \"https://github.com/genaiatutd/Spring-2025-Chatbot/raw/main/Gen-AI-session-20250210T210052Z-001.zip\"\n",
        "\n",
        "output_file = \"chatbot_files.zip\"\n",
        "\n",
        "# Download the ZIP file\n",
        "response = requests.get(zip_file_url, stream=True)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open(output_file, \"wb\") as file:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            file.write(chunk)\n",
        "    print(f\"Downloaded: {output_file}\")\n",
        "\n",
        "    # Extract the ZIP file\n",
        "    extracted_folder = \"chatbot_content\"\n",
        "    with zipfile.ZipFile(output_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_folder)\n",
        "    print(f\"Extraction complete. Contents are in '{extracted_folder}/'\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to download file. HTTP Status Code: {response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkRpyT0dg0j6",
        "outputId": "a16c107a-def5-411b-823b-e0740aedc94a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: chatbot_files.zip\n",
            "Extraction complete. Contents are in 'chatbot_content/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import hub\n",
        "import openai\n",
        "\n",
        "# ðŸ”¹ Set up OpenAI API key\n",
        "openai.api_key = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "hksiKGX023aG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Dataset setup**"
      ],
      "metadata": {
        "id": "bX-6Nd4BRtaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Load the robot image\n",
        "image_path = \"/content/chatbot_content/Gen-AI-session/my_robot.png\"\n",
        "robot_image = Image.open(image_path).resize((900, 300))\n",
        "\n",
        "# ðŸ”¹ Set directories\n",
        "pdf_directory = \"/content/chatbot_content/Gen-AI-session\"\n",
        "persist_directory = \"/content/vectorstore\"\n",
        "\n",
        "# ðŸ”¹ Ensure persist directory exists\n",
        "if not os.path.exists(persist_directory):\n",
        "    os.makedirs(persist_directory)\n",
        "\n",
        "# ðŸ”¹ Load PDFs into LangChain Document Loader\n",
        "all_docs = []\n",
        "for file in os.listdir(pdf_directory):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        loader = PyPDFLoader(os.path.join(pdf_directory, file))\n",
        "        docs = loader.load()\n",
        "        all_docs.extend(docs)\n",
        "\n",
        "print(f\"ðŸ“„ Loaded {len(all_docs)} pages from PDFs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeEV2BQtykjR",
        "outputId": "ce899f11-b85b-478b-e0be-893decf86994"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“„ Loaded 32 pages from PDFs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Split the PDFs into data chunks**"
      ],
      "metadata": {
        "id": "b_5H-CYbTAqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Split text into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(all_docs)\n",
        "\n",
        "print(f\"ðŸ“‘ Split into {len(chunks)} document chunks.\")\n",
        "\n",
        "# ðŸ”¹ Store documents in Chroma\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(chunks, embedding=embedding_function, persist_directory=persist_directory)\n",
        "\n",
        "print(f\"âœ… Successfully stored {len(vectorstore.get()['documents'])} document chunks in Chroma!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHDd-HdkRQoN",
        "outputId": "60195092-53bc-428e-a07c-8af5ffff6039"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‘ Split into 131 document chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-e8f2c95d5dd2>:8: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding_function = OpenAIEmbeddings()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Successfully stored 131 document chunks in Chroma!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Initiate RAG ***"
      ],
      "metadata": {
        "id": "95Pvo7INTFC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Define RAG framework components\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Retrieves top 3 most relevant chunks\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# ðŸ”¹ Function to format retrieved documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs if doc.page_content.strip())\n",
        "\n",
        "# ðŸ”¹ Define RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# ðŸ”¹ Initialize conversation history\n",
        "conversation_history = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZQUmU2zRZg3",
        "outputId": "47129002-2503-433f-b52a-87d029fabefd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Response function to provide contextually rich answers**"
      ],
      "metadata": {
        "id": "TXRP9Ac5TNqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Function to generate chatbot responses\n",
        "def generate_response(question):\n",
        "    global conversation_history\n",
        "\n",
        "    # Ensure question is valid\n",
        "    if not question or not isinstance(question, str):\n",
        "        return \"Invalid input. Please ask a relevant question.\"\n",
        "\n",
        "    question_lower = question.lower().strip()\n",
        "\n",
        "    # Predefined quick responses\n",
        "    greetings = {\"hi\", \"hello\", \"hey\", \"hola\"}\n",
        "    thanks = {\"Thank you!\",\"thank you\",\"Thanks!\", \"thanks\", \"thx\", \"ty!\"}\n",
        "\n",
        "    if question_lower in greetings:\n",
        "        return \"Hello! How can I assist you today?\"\n",
        "    elif question_lower in thanks:\n",
        "        return \"You're welcome! Let me know if you need more help.\"\n",
        "\n",
        "    # Retrieve context from Chroma\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    formatted_context = format_docs(retrieved_docs)\n",
        "\n",
        "    # If no context is found, return a relevant response\n",
        "    if not formatted_context.strip():\n",
        "        return \"I couldn't find relevant information in my database. Can you clarify or provide more details?\"\n",
        "\n",
        "    # Prepare the input for RAG model\n",
        "    full_input = f\"Context:\\n{formatted_context}\\n\\nQuestion: {question}\"\n",
        "\n",
        "    # Generate response using the RAG chain\n",
        "    response = rag_chain.invoke(full_input)\n",
        "\n",
        "    # Append conversation history\n",
        "    conversation_history.append(f\"User: {question}\\nBot: {response}\")\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "3WDK-m-jRcT2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. User Interface**"
      ],
      "metadata": {
        "id": "kleJR5-OS_Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Define Gradio Chatbot UI\n",
        "with gr.Blocks() as iface:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"<h1 style='text-align: center;'>UTD Chatbot</h1>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"<p style='text-align: center;'><b>Ask any question about UTD!</b></p>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        gr.Image(value=robot_image, label=\"UTD Chatbot\", show_label=False)\n",
        "\n",
        "    with gr.Column(scale=1, min_width=300):\n",
        "        input_box = gr.Textbox(label=\"Ask your question\", placeholder=\"Type here...\")\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "        clear_btn = gr.Button(\"Clear\", variant=\"secondary\")\n",
        "\n",
        "    output_box = gr.Textbox(label=\"Response\", interactive=False)\n",
        "\n",
        "    submit_btn.click(fn=generate_response, inputs=input_box, outputs=output_box)\n",
        "    clear_btn.click(lambda: (\"\", \"\"), inputs=[], outputs=[input_box, output_box])\n",
        "\n",
        "iface.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "hZRljUw0Repd",
        "outputId": "dd8cce2b-eee8-4c5f-b51d-dc3de4cae868"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a4a12748ca4f3e3ba6.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a4a12748ca4f3e3ba6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a4a12748ca4f3e3ba6.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_-GgR7SrjIOG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}